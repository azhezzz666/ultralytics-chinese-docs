# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

from __future__ import annotations

import time

import cv2

from ultralytics import YOLO
from ultralytics.utils import LOGGER
from ultralytics.utils.plotting import Annotator, colors

enable_gpu = False  # å¦‚æœä½¿ç”¨ CUDA è¿è¡Œï¼Œè®¾ç½®ä¸º True
model_file = "yolo11s.pt"  # æ¨¡å‹æ–‡ä»¶è·¯å¾„
show_fps = True  # å¦‚æœä¸º Trueï¼Œåœ¨å·¦ä¸Šè§’æ˜¾ç¤ºå½“å‰ FPS
show_conf = False  # æ˜¾ç¤ºæˆ–éšè—ç½®ä¿¡åº¦åˆ†æ•°
save_video = False  # è®¾ç½®ä¸º True ä»¥ä¿å­˜è¾“å‡ºè§†é¢‘
video_output_path = "interactive_tracker_output.avi"  # è¾“å‡ºè§†é¢‘æ–‡ä»¶å


conf = 0.3  # ç›®æ ‡æ£€æµ‹çš„æœ€å°ç½®ä¿¡åº¦ï¼ˆè¾ƒä½ = æ›´å¤šæ£€æµ‹ï¼Œå¯èƒ½æ›´å¤šè¯¯æ£€ï¼‰
iou = 0.3  # NMS çš„ IoU é˜ˆå€¼ï¼ˆè¾ƒé«˜ = å…è®¸æ›´å°‘é‡å ï¼‰
max_det = 20  # æ¯å¼ å›¾åƒçš„æœ€å¤§ç›®æ ‡æ•°ï¼ˆå¯¹äºæ‹¥æŒ¤åœºæ™¯å¯å¢åŠ ï¼‰

tracker = "bytetrack.yaml"  # è·Ÿè¸ªå™¨é…ç½®: 'bytetrack.yaml', 'botsort.yaml' ç­‰
track_args = {
    "persist": True,  # ä¿æŒå¸§å†å²ä½œä¸ºæµä»¥è¿›è¡Œè¿ç»­è·Ÿè¸ª
    "verbose": False,  # æ‰“å°è·Ÿè¸ªå™¨çš„è°ƒè¯•ä¿¡æ¯
}

window_name = "Ultralytics YOLO äº¤äº’å¼è·Ÿè¸ª"  # è¾“å‡ºçª—å£åç§°

LOGGER.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
if enable_gpu:
    LOGGER.info("ä½¿ç”¨ GPU...")
    model = YOLO(model_file)
    model.to("cuda")
else:
    LOGGER.info("ä½¿ç”¨ CPU...")
    model = YOLO(model_file, task="detect")

classes = model.names  # å­˜å‚¨æ¨¡å‹ç±»åˆ«åç§°

cap = cv2.VideoCapture(0)  # å¦‚éœ€è¦å¯æ›¿æ¢ä¸ºè§†é¢‘è·¯å¾„
if not cap.isOpened():
    raise SystemError("æ— æ³•æ‰“å¼€è§†é¢‘æºã€‚")

vw = None  # åœ¨è¯»å–ç¬¬ä¸€å¸§åå»¶è¿Ÿåˆå§‹åŒ–

selected_object_id = None
selected_bbox = None
selected_center = None
latest_detections: list[list[float]] = []


def get_center(x1: int, y1: int, x2: int, y2: int) -> tuple[int, int]:
    """è®¡ç®—è¾¹ç•Œæ¡†çš„ä¸­å¿ƒç‚¹ã€‚

    Args:
        x1 (int): å·¦ä¸Šè§’ X åæ ‡ã€‚
        y1 (int): å·¦ä¸Šè§’ Y åæ ‡ã€‚
        x2 (int): å³ä¸‹è§’ X åæ ‡ã€‚
        y2 (int): å³ä¸‹è§’ Y åæ ‡ã€‚

    Returns:
        center_x (int): ä¸­å¿ƒç‚¹çš„ X åæ ‡ã€‚
        center_y (int): ä¸­å¿ƒç‚¹çš„ Y åæ ‡ã€‚
    """
    return (x1 + x2) // 2, (y1 + y2) // 2


def extend_line_from_edge(mid_x: int, mid_y: int, direction: str, img_shape: tuple[int, int, int]) -> tuple[int, int]:
    """è®¡ç®—ä»ä¸­å¿ƒå‘å›¾åƒè¾¹ç¼˜å»¶ä¼¸çº¿çš„ç«¯ç‚¹ã€‚

    Args:
        mid_x (int): ä¸­ç‚¹çš„ X åæ ‡ã€‚
        mid_y (int): ä¸­ç‚¹çš„ Y åæ ‡ã€‚
        direction (str): å»¶ä¼¸æ–¹å‘ ('left', 'right', 'up', 'down')ã€‚
        img_shape (tuple[int, int, int]): å›¾åƒå½¢çŠ¶ (é«˜åº¦, å®½åº¦, é€šé“)ã€‚

    Returns:
        end_x (int): ç«¯ç‚¹çš„ X åæ ‡ã€‚
        end_y (int): ç«¯ç‚¹çš„ Y åæ ‡ã€‚
    """
    h, w = img_shape[:2]
    if direction == "down":
        return mid_x, h - 1
    elif direction == "left":
        return 0, mid_y
    elif direction == "right":
        return w - 1, mid_y
    elif direction == "up":
        return mid_x, 0
    else:
        return mid_x, mid_y


def draw_tracking_scope(im, bbox: tuple, color: tuple) -> None:
    """ç»˜åˆ¶ä»è¾¹ç•Œæ¡†å»¶ä¼¸åˆ°å›¾åƒè¾¹ç¼˜çš„è·Ÿè¸ªèŒƒå›´çº¿ã€‚

    Args:
        im (np.ndarray): è¦ç»˜åˆ¶çš„å›¾åƒæ•°ç»„ã€‚
        bbox (tuple): è¾¹ç•Œæ¡†åæ ‡ (x1, y1, x2, y2)ã€‚
        color (tuple): ç»˜åˆ¶ç”¨çš„ BGR æ ¼å¼é¢œè‰²ã€‚
    """
    x1, y1, x2, y2 = bbox
    mid_top = ((x1 + x2) // 2, y1)
    mid_bottom = ((x1 + x2) // 2, y2)
    mid_left = (x1, (y1 + y2) // 2)
    mid_right = (x2, (y1 + y2) // 2)
    cv2.line(im, mid_top, extend_line_from_edge(*mid_top, "up", im.shape), color, 2)
    cv2.line(im, mid_bottom, extend_line_from_edge(*mid_bottom, "down", im.shape), color, 2)
    cv2.line(im, mid_left, extend_line_from_edge(*mid_left, "left", im.shape), color, 2)
    cv2.line(im, mid_right, extend_line_from_edge(*mid_right, "right", im.shape), color, 2)


def click_event(event: int, x: int, y: int, flags: int, param) -> None:
    """å¤„ç†é¼ æ ‡ç‚¹å‡»äº‹ä»¶ä»¥é€‰æ‹©è¦èšç„¦è·Ÿè¸ªçš„ç›®æ ‡ã€‚

    Args:
        event (int): OpenCV é¼ æ ‡äº‹ä»¶ç±»å‹ã€‚
        x (int): é¼ æ ‡äº‹ä»¶çš„ X åæ ‡ã€‚
        y (int): é¼ æ ‡äº‹ä»¶çš„ Y åæ ‡ã€‚
        flags (int): OpenCV ä¼ é€’çš„ä»»ä½•ç›¸å…³æ ‡å¿—ã€‚
        param (Any): é™„åŠ å‚æ•°ï¼ˆæœªä½¿ç”¨ï¼‰ã€‚
    """
    global selected_object_id, latest_detections
    if event == cv2.EVENT_LBUTTONDOWN:
        if not latest_detections:
            return
        min_area = float("inf")
        best_match = None
        for track in latest_detections:
            if len(track) < 6:
                continue
            x1, y1, x2, y2 = map(int, track[:4])
            if x1 <= x <= x2 and y1 <= y <= y2:
                area = max(0, x2 - x1) * max(0, y2 - y1)
                if area < min_area:
                    track_id = int(track[4]) if len(track) >= 7 else -1
                    class_id = int(track[6]) if len(track) >= 7 else int(track[5])
                    min_area = area
                    best_match = (track_id, classes.get(class_id, str(class_id)))
        if best_match:
            selected_object_id, label = best_match
            LOGGER.info(f"å¼€å§‹è·Ÿè¸ª: {label} (ID {selected_object_id})")


cv2.namedWindow(window_name)
cv2.setMouseCallback(window_name, click_event)

fps_counter, fps_timer, fps_display = 0, time.time(), 0

while cap.isOpened():
    success, im = cap.read()
    if not success:
        break

    results = model.track(im, conf=conf, iou=iou, max_det=max_det, tracker=tracker, **track_args)
    annotator = Annotator(im)
    detections = results[0].boxes.data if results[0].boxes is not None else []
    latest_detections = detections.cpu().tolist() if hasattr(detections, "cpu") else list(detections)  # type: ignore[arg-type]
    detected_objects: list[str] = []
    for track in detections:
        track = track.tolist()
        if len(track) < 6:
            continue
        x1, y1, x2, y2 = map(int, track[:4])
        class_id = int(track[6]) if len(track) >= 7 else int(track[5])
        track_id = int(track[4]) if len(track) == 7 else -1
        color = colors(track_id, True)
        txt_color = annotator.get_txt_color(color)
        conf_score = float(track[5]) if len(track) >= 7 else 0.0
        class_name = classes.get(class_id, str(class_id))
        label = f"{class_name} ID {track_id}" + (f" ({conf_score:.2f})" if show_conf else "")
        center = get_center(x1, y1, x2, y2)
        detected_objects.append(f"{class_name}#{track_id}@{center[0]},{center[1]}")
        if track_id == selected_object_id:
            draw_tracking_scope(im, (x1, y1, x2, y2), color)
            cv2.circle(im, center, 6, color, -1)

            # ç”¨äºå¸å¼•æ³¨æ„åŠ›çš„è„‰å†²åœ†
            pulse_radius = 8 + int(4 * abs(time.time() % 1 - 0.5))
            cv2.circle(im, center, pulse_radius, color, 2)

            annotator.box_label([x1, y1, x2, y2], label=f"æ¿€æ´»: è·Ÿè¸ª {track_id}", color=color)
        else:
            # ä¸ºå…¶ä»–ç›®æ ‡ç»˜åˆ¶è™šçº¿æ¡†
            for i in range(x1, x2, 10):
                cv2.line(im, (i, y1), (i + 5, y1), color, 3)
                cv2.line(im, (i, y2), (i + 5, y2), color, 3)
            for i in range(y1, y2, 10):
                cv2.line(im, (x1, i), (x1, i + 5), color, 3)
                cv2.line(im, (x2, i), (x2, i + 5), color, 3)
            # ç»˜åˆ¶å¸¦èƒŒæ™¯çš„æ ‡ç­¾æ–‡æœ¬
            (tw, th), bl = cv2.getTextSize(label, 0, 0.7, 2)
            cv2.rectangle(im, (x1 + 5 - 5, y1 + 20 - th - 5), (x1 + 5 + tw + 5, y1 + 20 + bl), color, -1)
            cv2.putText(im, label, (x1 + 5, y1 + 20), 0, 0.7, txt_color, 1, cv2.LINE_AA)

    if show_fps:
        fps_counter += 1
        if time.time() - fps_timer >= 1.0:
            fps_display = fps_counter
            fps_counter = 0
            fps_timer = time.time()

        # ç»˜åˆ¶å¸¦èƒŒæ™¯çš„ FPS æ–‡æœ¬
        fps_text = f"FPS: {fps_display}"
        (tw, th), bl = cv2.getTextSize(fps_text, 0, 0.7, 2)
        cv2.rectangle(im, (10 - 5, 25 - th - 5), (10 + tw + 5, 25 + bl), (255, 255, 255), -1)
        cv2.putText(im, fps_text, (10, 25), 0, 0.7, (104, 31, 17), 1, cv2.LINE_AA)

    if save_video and vw is None:
        h, w = im.shape[:2]
        fps = cap.get(cv2.CAP_PROP_FPS) or 0
        fps = float(fps) if fps and fps > 0 else 30.0
        ext = video_output_path.lower()
        fourcc = cv2.VideoWriter_fourcc(*("MJPG" if ext.endswith(".avi") else "mp4v"))
        vw = cv2.VideoWriter(video_output_path, fourcc, fps, (w, h))

    cv2.imshow(window_name, im)
    if save_video and vw is not None:
        vw.write(im)
    # ç»ˆç«¯æ—¥å¿—è®°å½•
    LOGGER.info(
        f"æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡: {' | '.join(detected_objects)}"
        if detected_objects
        else f"æ£€æµ‹åˆ° {len(detections)} ä¸ªç›®æ ‡ã€‚"
    )

    key = cv2.waitKey(1) & 0xFF
    if key == ord("q"):
        break
    elif key == ord("c"):
        LOGGER.info("è·Ÿè¸ªå·²é‡ç½®ã€‚")
        selected_object_id = None

cap.release()
if save_video and vw is not None:
    vw.release()
cv2.destroyAllWindows()
