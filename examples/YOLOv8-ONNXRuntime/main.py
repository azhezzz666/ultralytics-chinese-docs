# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

from __future__ import annotations

import argparse

import cv2
import numpy as np
import onnxruntime as ort
import torch

from ultralytics.utils import ASSETS, YAML
from ultralytics.utils.checks import check_requirements, check_yaml


class YOLOv8:
    """YOLOv8 ç›®æ ‡æ£€æµ‹æ¨¡å‹ç±»ï¼Œç”¨äºå¤„ç† ONNX æ¨ç†å’Œå¯è§†åŒ–ã€‚

    è¯¥ç±»æä¾›åŠ è½½ YOLOv8 ONNX æ¨¡å‹ã€å¯¹å›¾åƒæ‰§è¡Œæ¨ç†ä»¥åŠä½¿ç”¨è¾¹ç•Œæ¡†å’Œæ ‡ç­¾å¯è§†åŒ–æ£€æµ‹ç»“æœçš„åŠŸèƒ½ã€‚

    Attributes:
        onnx_model (str): ONNX æ¨¡å‹æ–‡ä»¶è·¯å¾„ã€‚
        input_image (str): è¾“å…¥å›¾åƒæ–‡ä»¶è·¯å¾„ã€‚
        confidence_thres (float): è¿‡æ»¤æ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼ã€‚
        iou_thres (float): éæå¤§å€¼æŠ‘åˆ¶çš„ IoU é˜ˆå€¼ã€‚
        classes (list[str]): COCO æ•°æ®é›†çš„ç±»åˆ«åç§°åˆ—è¡¨ã€‚
        color_palette (np.ndarray): ç”¨äºå¯è§†åŒ–ä¸åŒç±»åˆ«çš„éšæœºé¢œè‰²è°ƒè‰²æ¿ã€‚
        input_width (int): æ¨¡å‹è¾“å…¥çš„å®½åº¦ç»´åº¦ã€‚
        input_height (int): æ¨¡å‹è¾“å…¥çš„é«˜åº¦ç»´åº¦ã€‚
        img (np.ndarray): åŠ è½½çš„è¾“å…¥å›¾åƒã€‚
        img_height (int): è¾“å…¥å›¾åƒçš„é«˜åº¦ã€‚
        img_width (int): è¾“å…¥å›¾åƒçš„å®½åº¦ã€‚

    Methods:
        letterbox: åœ¨ä¿æŒå®½é«˜æ¯”çš„åŒæ—¶é€šè¿‡æ·»åŠ å¡«å……æ¥è°ƒæ•´å’Œé‡å¡‘å›¾åƒã€‚
        draw_detections: æ ¹æ®æ£€æµ‹åˆ°çš„ç›®æ ‡åœ¨è¾“å…¥å›¾åƒä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†å’Œæ ‡ç­¾ã€‚
        preprocess: åœ¨æ‰§è¡Œæ¨ç†ä¹‹å‰é¢„å¤„ç†è¾“å…¥å›¾åƒã€‚
        postprocess: å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œåå¤„ç†ä»¥æå–å’Œå¯è§†åŒ–æ£€æµ‹ç»“æœã€‚
        main: ä½¿ç”¨ ONNX æ¨¡å‹æ‰§è¡Œæ¨ç†å¹¶è¿”å›å¸¦æœ‰ç»˜åˆ¶æ£€æµ‹ç»“æœçš„è¾“å‡ºå›¾åƒã€‚

    Examples:
        åˆå§‹åŒ– YOLOv8 æ£€æµ‹å™¨å¹¶è¿è¡Œæ¨ç†
        >>> detector = YOLOv8("yolov8n.onnx", "image.jpg", 0.5, 0.5)
        >>> output_image = detector.main()
    """

    def __init__(self, onnx_model: str, input_image: str, confidence_thres: float, iou_thres: float):
        """åˆå§‹åŒ– YOLOv8 ç±»çš„å®ä¾‹ã€‚

        Args:
            onnx_model (str): ONNX æ¨¡å‹çš„è·¯å¾„ã€‚
            input_image (str): è¾“å…¥å›¾åƒçš„è·¯å¾„ã€‚
            confidence_thres (float): è¿‡æ»¤æ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼ã€‚
            iou_thres (float): éæå¤§å€¼æŠ‘åˆ¶çš„ IoU é˜ˆå€¼ã€‚
        """
        self.onnx_model = onnx_model
        self.input_image = input_image
        self.confidence_thres = confidence_thres
        self.iou_thres = iou_thres

        # ä» COCO æ•°æ®é›†åŠ è½½ç±»åˆ«åç§°
        self.classes = YAML.load(check_yaml("coco8.yaml"))["names"]

        # ä¸ºç±»åˆ«ç”Ÿæˆé¢œè‰²è°ƒè‰²æ¿
        self.color_palette = np.random.uniform(0, 255, size=(len(self.classes), 3))

    def letterbox(self, img: np.ndarray, new_shape: tuple[int, int] = (640, 640)) -> tuple[np.ndarray, tuple[int, int]]:
        """åœ¨ä¿æŒå®½é«˜æ¯”çš„åŒæ—¶é€šè¿‡æ·»åŠ å¡«å……æ¥è°ƒæ•´å’Œé‡å¡‘å›¾åƒã€‚

        Args:
            img (np.ndarray): è¦è°ƒæ•´å¤§å°çš„è¾“å…¥å›¾åƒã€‚
            new_shape (tuple[int, int]): å›¾åƒçš„ç›®æ ‡å½¢çŠ¶ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ã€‚

        Returns:
            img (np.ndarray): è°ƒæ•´å¤§å°å¹¶å¡«å……åçš„å›¾åƒã€‚
            pad (tuple[int, int]): åº”ç”¨äºå›¾åƒçš„å¡«å……å€¼ï¼ˆé¡¶éƒ¨ï¼Œå·¦ä¾§ï¼‰ã€‚
        """
        shape = img.shape[:2]  # å½“å‰å½¢çŠ¶ [é«˜åº¦, å®½åº¦]

        # ç¼©æ”¾æ¯”ä¾‹ï¼ˆæ–° / æ—§ï¼‰
        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])

        # è®¡ç®—å¡«å……
        new_unpad = round(shape[1] * r), round(shape[0] * r)
        dw, dh = (new_shape[1] - new_unpad[0]) / 2, (new_shape[0] - new_unpad[1]) / 2  # å®½é«˜å¡«å……

        if shape[::-1] != new_unpad:  # è°ƒæ•´å¤§å°
            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
        top, bottom = round(dh - 0.1), round(dh + 0.1)
        left, right = round(dw - 0.1), round(dw + 0.1)
        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))

        return img, (top, left)

    def draw_detections(self, img: np.ndarray, box: list[float], score: float, class_id: int) -> None:
        """æ ¹æ®æ£€æµ‹åˆ°çš„ç›®æ ‡åœ¨è¾“å…¥å›¾åƒä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†å’Œæ ‡ç­¾ã€‚"""
        # æå–è¾¹ç•Œæ¡†çš„åæ ‡
        x1, y1, w, h = box

        # è·å–ç±»åˆ« ID å¯¹åº”çš„é¢œè‰²
        color = self.color_palette[class_id]

        # åœ¨å›¾åƒä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(img, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)

        # åˆ›å»ºåŒ…å«ç±»åˆ«åç§°å’Œåˆ†æ•°çš„æ ‡ç­¾æ–‡æœ¬
        label = f"{self.classes[class_id]}: {score:.2f}"

        # è®¡ç®—æ ‡ç­¾æ–‡æœ¬çš„å°ºå¯¸
        (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)

        # è®¡ç®—æ ‡ç­¾æ–‡æœ¬çš„ä½ç½®
        label_x = x1
        label_y = y1 - 10 if y1 - 10 > label_height else y1 + 10

        # ç»˜åˆ¶å¡«å……çŸ©å½¢ä½œä¸ºæ ‡ç­¾æ–‡æœ¬çš„èƒŒæ™¯
        cv2.rectangle(
            img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height), color, cv2.FILLED
        )

        # åœ¨å›¾åƒä¸Šç»˜åˆ¶æ ‡ç­¾æ–‡æœ¬
        cv2.putText(img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)

    def preprocess(self) -> tuple[np.ndarray, tuple[int, int]]:
        """åœ¨æ‰§è¡Œæ¨ç†ä¹‹å‰é¢„å¤„ç†è¾“å…¥å›¾åƒã€‚

        è¯¥æ–¹æ³•è¯»å–è¾“å…¥å›¾åƒï¼Œè½¬æ¢é¢œè‰²ç©ºé—´ï¼Œåº”ç”¨ letterbox ä¿æŒå®½é«˜æ¯”ï¼Œ
        å½’ä¸€åŒ–åƒç´ å€¼ï¼Œå¹¶ä¸ºæ¨¡å‹è¾“å…¥å‡†å¤‡å›¾åƒæ•°æ®ã€‚

        Returns:
            image_data (np.ndarray): å‡†å¤‡å¥½è¿›è¡Œæ¨ç†çš„é¢„å¤„ç†å›¾åƒæ•°æ®ï¼Œå½¢çŠ¶ä¸º (1, 3, height, width)ã€‚
            pad (tuple[int, int]): letterbox è¿‡ç¨‹ä¸­åº”ç”¨çš„å¡«å……å€¼ï¼ˆé¡¶éƒ¨ï¼Œå·¦ä¾§ï¼‰ã€‚
        """
        # ä½¿ç”¨ OpenCV è¯»å–è¾“å…¥å›¾åƒ
        self.img = cv2.imread(self.input_image)

        # è·å–è¾“å…¥å›¾åƒçš„é«˜åº¦å’Œå®½åº¦
        self.img_height, self.img_width = self.img.shape[:2]

        # å°†å›¾åƒé¢œè‰²ç©ºé—´ä» BGR è½¬æ¢ä¸º RGB
        img = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)

        img, pad = self.letterbox(img, (self.input_width, self.input_height))

        # é€šè¿‡é™¤ä»¥ 255.0 å½’ä¸€åŒ–å›¾åƒæ•°æ®
        image_data = np.array(img) / 255.0

        # è½¬ç½®å›¾åƒï¼Œä½¿é€šé“ç»´åº¦æˆä¸ºç¬¬ä¸€ä¸ªç»´åº¦
        image_data = np.transpose(image_data, (2, 0, 1))  # é€šé“ä¼˜å…ˆ

        # æ‰©å±•å›¾åƒæ•°æ®çš„ç»´åº¦ä»¥åŒ¹é…é¢„æœŸçš„è¾“å…¥å½¢çŠ¶
        image_data = np.expand_dims(image_data, axis=0).astype(np.float32)

        # è¿”å›é¢„å¤„ç†åçš„å›¾åƒæ•°æ®
        return image_data, pad

    def postprocess(self, input_image: np.ndarray, output: list[np.ndarray], pad: tuple[int, int]) -> np.ndarray:
        """å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œåå¤„ç†ä»¥æå–å’Œå¯è§†åŒ–æ£€æµ‹ç»“æœã€‚

        è¯¥æ–¹æ³•å¤„ç†åŸå§‹æ¨¡å‹è¾“å‡ºä»¥æå–è¾¹ç•Œæ¡†ã€åˆ†æ•°å’Œç±»åˆ« IDã€‚å®ƒåº”ç”¨éæå¤§å€¼æŠ‘åˆ¶
        æ¥è¿‡æ»¤é‡å çš„æ£€æµ‹ï¼Œå¹¶åœ¨è¾“å…¥å›¾åƒä¸Šç»˜åˆ¶ç»“æœã€‚

        Args:
            input_image (np.ndarray): è¾“å…¥å›¾åƒã€‚
            output (list[np.ndarray]): æ¨¡å‹çš„è¾“å‡ºæ•°ç»„ã€‚
            pad (tuple[int, int]): letterbox è¿‡ç¨‹ä¸­ä½¿ç”¨çš„å¡«å……å€¼ï¼ˆé¡¶éƒ¨ï¼Œå·¦ä¾§ï¼‰ã€‚

        Returns:
            (np.ndarray): ç»˜åˆ¶äº†æ£€æµ‹ç»“æœçš„è¾“å…¥å›¾åƒã€‚
        """
        # è½¬ç½®å¹¶å‹ç¼©è¾“å‡ºä»¥åŒ¹é…é¢„æœŸå½¢çŠ¶
        outputs = np.transpose(np.squeeze(output[0]))

        # è·å–è¾“å‡ºæ•°ç»„çš„è¡Œæ•°
        rows = outputs.shape[0]

        # ç”¨äºå­˜å‚¨æ£€æµ‹çš„è¾¹ç•Œæ¡†ã€åˆ†æ•°å’Œç±»åˆ« ID çš„åˆ—è¡¨
        boxes = []
        scores = []
        class_ids = []

        # è®¡ç®—è¾¹ç•Œæ¡†åæ ‡çš„ç¼©æ”¾å› å­
        gain = min(self.input_height / self.img_height, self.input_width / self.img_width)
        outputs[:, 0] -= pad[1]
        outputs[:, 1] -= pad[0]

        # éå†è¾“å‡ºæ•°ç»„ä¸­çš„æ¯ä¸€è¡Œ
        for i in range(rows):
            # ä»å½“å‰è¡Œæå–ç±»åˆ«åˆ†æ•°
            classes_scores = outputs[i][4:]

            # æ‰¾åˆ°ç±»åˆ«åˆ†æ•°ä¸­çš„æœ€å¤§å€¼
            max_score = np.amax(classes_scores)

            # å¦‚æœæœ€å¤§åˆ†æ•°é«˜äºç½®ä¿¡åº¦é˜ˆå€¼
            if max_score >= self.confidence_thres:
                # è·å–åˆ†æ•°æœ€é«˜çš„ç±»åˆ« ID
                class_id = np.argmax(classes_scores)

                # ä»å½“å‰è¡Œæå–è¾¹ç•Œæ¡†åæ ‡
                x, y, w, h = outputs[i][0], outputs[i][1], outputs[i][2], outputs[i][3]

                # è®¡ç®—è¾¹ç•Œæ¡†çš„ç¼©æ”¾åæ ‡
                left = int((x - w / 2) / gain)
                top = int((y - h / 2) / gain)
                width = int(w / gain)
                height = int(h / gain)

                # å°†ç±»åˆ« IDã€åˆ†æ•°å’Œè¾¹ç•Œæ¡†åæ ‡æ·»åŠ åˆ°ç›¸åº”çš„åˆ—è¡¨
                class_ids.append(class_id)
                scores.append(max_score)
                boxes.append([left, top, width, height])

        # åº”ç”¨éæå¤§å€¼æŠ‘åˆ¶æ¥è¿‡æ»¤é‡å çš„è¾¹ç•Œæ¡†
        indices = cv2.dnn.NMSBoxes(boxes, scores, self.confidence_thres, self.iou_thres)

        # éå†éæå¤§å€¼æŠ‘åˆ¶åé€‰æ‹©çš„ç´¢å¼•
        for i in np.array(indices).flatten():
            # è·å–å¯¹åº”ç´¢å¼•çš„è¾¹ç•Œæ¡†ã€åˆ†æ•°å’Œç±»åˆ« ID
            box = boxes[int(i)]
            score = scores[int(i)]
            class_id = class_ids[int(i)]

            # åœ¨è¾“å…¥å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ
            self.draw_detections(input_image, box, score, class_id)

        # è¿”å›ä¿®æ”¹åçš„è¾“å…¥å›¾åƒ
        return input_image

    def main(self) -> np.ndarray:
        """ä½¿ç”¨ ONNX æ¨¡å‹æ‰§è¡Œæ¨ç†å¹¶è¿”å›å¸¦æœ‰ç»˜åˆ¶æ£€æµ‹ç»“æœçš„è¾“å‡ºå›¾åƒã€‚

        Returns:
            (np.ndarray): å¸¦æœ‰ç»˜åˆ¶æ£€æµ‹ç»“æœçš„è¾“å‡ºå›¾åƒã€‚
        """
        available = ort.get_available_providers()
        providers = [p for p in ("CUDAExecutionProvider", "CPUExecutionProvider") if p in available]
        session = ort.InferenceSession(self.onnx_model, providers=providers or available)

        # è·å–æ¨¡å‹è¾“å…¥
        model_inputs = session.get_inputs()

        # å­˜å‚¨è¾“å…¥å½¢çŠ¶ä»¥ä¾›åç»­ä½¿ç”¨
        input_shape = model_inputs[0].shape
        self.input_width = input_shape[2]
        self.input_height = input_shape[3]

        # é¢„å¤„ç†å›¾åƒæ•°æ®
        img_data, pad = self.preprocess()

        # ä½¿ç”¨é¢„å¤„ç†åçš„å›¾åƒæ•°æ®è¿è¡Œæ¨ç†
        outputs = session.run(None, {model_inputs[0].name: img_data})

        # å¯¹è¾“å‡ºè¿›è¡Œåå¤„ç†ä»¥è·å¾—è¾“å‡ºå›¾åƒ
        return self.postprocess(self.img, outputs, pad)


if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨æ¥å¤„ç†å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, default="yolov8n.onnx", help="è¾“å…¥æ‚¨çš„ ONNX æ¨¡å‹ã€‚")
    parser.add_argument("--img", type=str, default=str(ASSETS / "bus.jpg"), help="è¾“å…¥å›¾åƒçš„è·¯å¾„ã€‚")
    parser.add_argument("--conf-thres", type=float, default=0.5, help="ç½®ä¿¡åº¦é˜ˆå€¼")
    parser.add_argument("--iou-thres", type=float, default=0.5, help="NMS IoU é˜ˆå€¼")
    args = parser.parse_args()

    # æ£€æŸ¥ä¾èµ–å¹¶é€‰æ‹©é€‚å½“çš„åç«¯ï¼ˆCPU æˆ– GPUï¼‰
    check_requirements("onnxruntime-gpu" if torch.cuda.is_available() else "onnxruntime")

    # ä½¿ç”¨æŒ‡å®šå‚æ•°åˆ›å»º YOLOv8 ç±»çš„å®ä¾‹
    detection = YOLOv8(args.model, args.img, args.conf_thres, args.iou_thres)

    # æ‰§è¡Œç›®æ ‡æ£€æµ‹å¹¶è·å–è¾“å‡ºå›¾åƒ
    output_image = detection.main()

    # åœ¨çª—å£ä¸­æ˜¾ç¤ºè¾“å‡ºå›¾åƒ
    cv2.namedWindow("Output", cv2.WINDOW_NORMAL)
    cv2.imshow("Output", output_image)

    # ç­‰å¾…æŒ‰é”®é€€å‡º
    cv2.waitKey(0)
